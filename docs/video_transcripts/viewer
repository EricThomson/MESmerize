This quickstart video tutorial is meant to give you an very simple overview of how you can progress from raw imaging data to interactive and traceable plots that can be shared with a Mesmerize project.

For this tutorial I will use a simple dataset from the Allen Institute, which consists of a 2p recording in the visual cortex while the mouse is be presented with sinusoidal gratings at various orientations. I decided to use something that most people are familiar with, however many features of Mesmerize which I'm about to show you can be applied to a broad range of experiments and organisms. For more details you can check out our paper and other videos.

When you open Mesmerize you get the Welcome Window. From here you can open a Viewer for viewing image sequences, a flowchart for analysis, and open or create a Mesmerize project. The `Help` menu has links to useful resources. If you want to report an issue, this takes you right to the issue tracker on GitHub. This will take you to the gitter. These will take you to the documentation... And at the bottom it displays the version of Mesmerize that you are currently running.

To start off, I'm going to create a simple Mesmerize project. You don't need to create a project right in the beginning for using many of the Viewer features in Mesmerize, such as the Caiman modules. However for this tutorial I will create one from the beginning so I can map stimulus information from the Allen dataset.

So let's click this to start a new project. Let's create it here, and I will name it "tutorial_example". The "finished" version of this project will be available for you to download, it's linked in the video description and on the docs website.

The stimuli in this dataset are sinusoidal grating that have 3 different features, the orientation of the gratings, a spatial frequency and temporal frequency. So I will create 3 Stimulus Type columns called "ori", "sf", and "tf". You can create as many `Stimulus or Behavior Type` columns as you want.

To later illustrate how ROIs can be annotated, I will add two ROI Type columns.
Let's add one column named "cell_type" and another named "anatomical_position". You can have as many of these "ROI Type columns" as you want, and they would usually carry categorical data.

Lastly, you can add additional "Custom" Categorical data Columns. These columns are used for storing information that relates to entire Samples, such as an animal's age, genotype, or strain. I will add a column to enter the age of the animal.

So now I'm done with the project configuration for this new project. You can modify the project configuration at any time in the future, you can add new Stimulus, ROI Type and Custom Columns at any point in the future.

You can read much more about the `Project Configuration` in the docs, you can open these docs directly from the `Help` menu -> New Project Help. And you will find detailed information about how to think about your project configuration.

OK now let's open a Viewer. The Mesmerize viewer is used for visualizing your imaging data, and interfaces with modules that allow you to annotate Regions of Interest, and stimulus or behavioral periods. It can also interface with various Caiman modules for motion correction & signal extraction. Alternatively, you may directly import regions of interest from Caiman or Suite2p output files.

To start off, there are two GUI modules for opening imaging data in the Viewer.

The tifffile GUI module lets you open 2D or 3D image sequences and the Mesfile module allows you to open imaging data captured by Femtonics microscopes. You can open images from any format for which there's a python library through the Viewer console. There is a separate video that describes how to do this.

To open a tiff file, first open the GUI from Modules -> Load images -> Tiff file. Alternatively, if you're on Linux or Mac OSX you can also drag & drop a tiff file into the Viewer window.

I'm going to load a file that contains imaging data from a piece of the pvc-7 Allen Brain Institute dataset.

Next, you must select a load method. "asarray" is usually the fastest and should work for most files. "imread" should work for all tiff files but can be slower. See the documentation link the description for more details on these load methods.
<-- Provide link in video description -->

You can set the axes order of the image file. Most tiff image files are in [t, x, y] order for 2D data, or  [t, z, x, y] for 3D data. If the axes order is different you can just enter that here.

Finally, we have the option of importing meta data from one of the listed formats. I have a JSON file which has the minimal amount of required meta data. It looks like this. Some pieces of information, such as the sampling rate of the recording, are necessary for certain downstream processing such as motion correction.
<-- Provide link in video description --> user_guides/viewer/modules/tiff_file.html#minimal-dict

If you have meta data in particular formats you can easily add functions for that and it can appear as an option in this list. For more information see the the documentation link in the description.
<-- Provide link in video description --> user_guides/viewer/modules/tiff_file.html#custom-functions

So the filename of my meta data file matches that of the tiff file so it's found automatically as you can see here, otherwise you can manually select it using the button.

Ok so now that we've selected the image & meta data we can load this image into the Viewer Work Environment.

Now we can use the pyqtgraph GUI to scroll through the video in time, zoom & pan around, and change the min-max levels of the displayed image.

There are a few simple tools that you can use with the image.

So from scrolling this video you can notice that our cells are on the left, we can crop this out.

As I mentioned earlier this dataset contains stimulus information. We can use the `Stimulus Mapping` Module to add the stimulus data to the Viewer Work Environment. Data entered through the Stimulus Mapping Module carries over in downstream analysis. The `Stimulus Mapping` module can also be used for mapping behavioral periods. You can of course just skip this if you're recording doesn't contain any stimulus or behavior information.

When you open the stimulus mapping module you get 3 tabs. These tabs correspond to the "Stimulus or Behavior Type" columns you set in you `Project Configuration`.
You can add stimulus periods manually by clicking the "Add Row" button. For example let's add one called "90" for "90 degrees" in the "orientation" page. You can choose a color for visualization in the Viewer plot.
Let's add two stimuli for "sf"...
And two for "tf".

Now you can click "Set all maps" to set the stimulus maps.
And you can choose which stimulus you want to visualize in the plot area.

All the stimulus information for this recording is contained in a csv file, so we can just import it using the script editor. This script basically loads the csv as a pandas dataframe, and splits it into 3 dataframes - one for orientation, spatial frequency and temporal frequency. The Mesmerize `Stimulus Mapping` Module is then able to directly take a dataframe for each of these 3 stimulus types. You'll find this script in the documentation for the `Stimulus Mapping` module.

So let's run the script, and now you can see all the stimulus periods have been loaded for all 3 stimulus types. And they can all be visualized in Viewer.

OK so back to the image. When scrolling through the video you can notice movement, so we can use the Caiman library to correct for this. First I'm going to create a new batch so we can batch process the motion correction with a few variants of parameters, and finally choose the best one. I'm going to create the batch on a fast filesystem since motion corrections steps write large files.

Now that we have a batch directory, let's open the Caiman Motion Correction module for parameter entry. I would highly recommend going through the official Caiman demos & documentation to understand the parameters so you can use this tool more effectively.

But briefly, the "max shifts X" and "max shifts Y" are upper limits for rigid motion correction of the video. One way to do this is to use the measure tool, which allows you to measure the distance between two points in pixel units. To use it click on the "Measure Tool" option in the menu, click a point on the image, and then click another point. The viewer status bar will show the distances whenever you hover over the measuring line.

And you can have as many measure lines as you want. <Draw a few>. I'll get rid of these, I need only one for now.

So you can scroll through and try to find a landmark that seems indicative of full-frame movement. This cell right here seems useful, so I'll try and get the line to span the maximum deviation.

So let's enter a max shifts x & y that's a bit higher than shown here.

Lets do 2 iterations of rigid correction.

The strides & overlaps parameters are for splitting the video into a grid and then performing motion correction in each box. The Caiman papers explain this in detail, which I highly recommend going through.

I'm going to try one variant with 196 & 98

And now let's add this to the batch.

You can see that this motion correction item has been added to the batch. When you click on it you can see all the information that is associated to this item. The first line is the UUID, a unique identifier for this batch item. And down here we can see all the motion correction parameters we entered.

OK, let's add another batch item with this image. I'm going to change the strides & overlaps a bit. 128 & 64.

And another, 96 * 48.

We can see all 3 batch items here, and you can how the parameters differ.

This process of adding batch items with several parameter variants can also be automated & performed much more quickly through the script editor. You can see the link in the description and the dedicated video for details.
<-- Provide link in video description -->

Now let's start this batch from the top.

When a batch is running, the batch items run in external processes. This allows you to do other things in Mesmerize while a batch is running. The standard out from that process is continuously updated here. You can set the maximum number of threads that batch items are allowed to use by going to System Configuration in the Welcome Window. For more information on the System Configuration settings see the documentation link in the description.
<-- Provide link in video description -->

This batch will take ~15 minutes to complete so I'll skip ahead.

All of our batch items are green, so that means they've completed successfully.

You can double click on a batch item to view the output. If we scroll through this video you can see that it's definitely less shaky.
Another way to see this is to get a mean projection, which you can do by going to Image -> Projections -> Mean.
So there's the mean projection of the motion corrected video.
We can also load the input for this motion correction item by clickign the "View Input" button in the batch manager. So as you can see from here we've loaded the input image, and let's get a mean projection of this as well.
So if you look closely at the motion corrected imaged mean & the input image mean you can see that the motion corrected mean is definitely sharper.

Let's check out our other batch items...

You may need to play around a bit with parameter combinations to determine what works best for your recordings. But as I mentioned previously, I highly recommend looking at the official Caiman documentation as well as their papers to really understand it.

So I'm going to pick this one and use it for CNMF.
Let's open the CNMF GUI, and again I highly recommend looking at the official Caiman demos and their documentation for details on the parameters.

Anyways, first I'm going to select a patch size in which neurons will be found. I'll use the measure tool...

There seem to be about 8 neurons per patch.

Let's use the measure tool again to estimate the radius of the neurons.

I'm going to try 3 different variants with different Signal to Noise Ratio values. Let's set this first one to 2.0.

Lastly, I'll create a manual ROI to estimate the decay time of a neuron. Open the ROI Manager. Right click the "Add ROI" button to add an elliptical ROI. Let's plot under a few neurons. This neuron looks good. We can zoom & pan through the plot to estimate the decay time... The recording is at 31 Hz so that gives us ... And I'll enter that as the decay time.

I'll copy & paste the name and add this to the batch.

If you look at the parameters for this item you can see that it's split into two parts; for CNMF and then for component evaluation. The Caiman resources explain this in more detail.

Note how the "fr", or framerate parameter, was automatically found. This is an example of why you must include meta data when initially loading your raw image data.

One last thing I'd like to change before starting these CNMF items is to reduce the number of threads that they will be allowed to use. This is because CNMF can be quite RAM intensive, and from experience I know that an image of this size will oversaturate the RAM I currently have available. By reducing the number of threads, it will also reduce the RAM usage. So I will set 30 threads.

OK let's select the first CNMF item and run the batch from here onward.
As usual, you will be able to view the standard out over here. And it will take ~5 minutes for each item.

------------------------------------------------------------

OK so these CNMF items have finished.

And you can add as many of these custom categorical data columns as you need. You can also add them to an existing project which already has Samples added to it. You'll find more details on this in the documentation.

So now let's take a look at these CNMF outputs.

So these CNMF items have all completed successfully. Let's take a look at them.

I'll open 3 viewers so we can look at all of them at once.

Just double click the CNMF items to open them in a Viewer.
When you have multiple viewers open you will be prompted for which viewer you want to use for loading the output of the batch item.

Let's dock this ROI Manager. Now I'll load the other 2.

OK so now we have all 3 open. We can see that the number of ROIs in each of them varies. This is because I had set one of the component evaluation parameters differently for these items. We can take a look at the parameters that we used through the console like before.

So for this one the rval_threshold is 0.6 and it has 70... ROIs

For this one the rval_threshold is ____ and it has ___ ROIs

and for this one the rval_thr is ....

OK I'm just going to pick this one for now.

When you click an ROI from the list the spatial localization and Curve are highlighted. And you have a few options for visualization.

You can annotate ROIs by tagging them with labels to each form of categorical data that you have defined in your project configuration.

This annotation process can also be automated through the console or script editor. This can be particularly useful if you want to map your ROIs, or cells, based on a reference atlas. See the dedicated video and documentation for details on how to do this.

OK so I'm just going to label a few of these ROIs. I'll just start from this one. I'll just tag this as "cortical" cell. When you press "Enter" on your keyboard it will take you to the next label. So now I'll tag anatomical_position as "anterior". And now you can press "Page down" on your keyboard to move to the next ROI.

While you're entering your ROI tags in this entry box, you can press the left and right arrow keys on your keyboard to play the video forward and backward. The Home & End keys will take you to the start & end of the video.

After you've annotated all of your information of interest you can add the current Viewer Work Environment as a *Sample* to your project. To do that go to File -> Add to Project. You're presented with an entry GUI. First fill out the Animal ID & Trial ID for this recording. Note that you cannot easily change the Animal ID & Trial ID after adding the Sample to the project. This is because they are partially used as identifiers for referring to the Sample in the project.

If you have defined any Custom categorical data columns you will also be presented to enter that information here. We have one column for animal age so I'll just enter something here.

Lastly you can add any comments you may have about this Sample. I usually use this to note any particularly interesting things about the recording, like if this Sample or Trial had particularly strong responses, or if the video was very noisy, and other sorts of comments.

OK now let's add this to the project...

And now we can open the project browser and see that the Sample is here. You can see the categorical labels, such as ROI tags and the "age".

Finally, as a simple demonstration of interactive plots let's load this data in a flowchart. I'll open a new flowchart from the welcome window. Right click to create a node for loading data. This node can be used to load data from the current Project. I'll Normalize these curves... and create a Heatmap.

The flowchart has a rich variety of nodes which are extensively documented and allow you to do quite an array of analysis. A few examples are also available on the website, and feel free to contact us on Gitter if you need help on performing a particular type of analysis in the flowchart.

Another example which I will demonstrate is to just perform peak detection. A peak detection example is also available in the docs.

So the Peak Detection node takes 3 inputs. It uses Derivatives for finding local maxima & minima. And it also takes Normalized curves and the actual curve data on which you want to place the peaks & bases.

The Normalized curves can be helpful for setting amplitude thresholds, which help to remove false-positive peaks.

There is a free online book by Prof. O'Haver which explains peak detection, and other aspects of signal processing. I've linked it in the description.

I'm going to use just a simple dF/Fo node for this example to use as the input curve on which we'll place peaks & bases. The data are quite noisy so I'll use a ButterWorth Filter. And then normalize the filtered curve, and get the derivative. Now let's feed these into the peak detection node.

Check the option "Fictional bases". This places a fictive base, or local minima, at the first & last index of each curve. You will often run into peak detection errors without this option. It will not effect downstream analysis as long as those bases are not associated with any peak.

Click apply and let's open the GUI to take a look at this.


